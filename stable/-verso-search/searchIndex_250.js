window.docContents[250].resolve({"/Basic-Types/Strings/#The-Lean-Language-Reference--Basic-Types--Strings--API-Reference--Raw-Substrings--Prefix-and-Suffix":{"contents":"Returns the longest common prefix of two substrings.The returned substring uses the same underlying string as s.\n\nReturns the longest common suffix of two substrings.The returned substring uses the same underlying string as s.\n\nIf pre is a prefix of s, returns the remainder. Returns none otherwise.The substring pre is a prefix of s if there exists a t : Substring such that\ns.toString = pre.toString ++ t.toString. If so, the result is the substring of s without the\nprefix.\n\nIf suff is a suffix of s, returns the remainder. Returns none otherwise.The substring suff is a suffix of s if there exists a t : Substring such that\ns.toString = t.toString ++ suff.toString. If so, the result the substring of s without the\nsuffix.\n\n","context":"Lean Reference\u0009Basic Types\u0009Strings\u0009API Reference\u0009Raw Substrings","header":"20.8.4.12.5. Prefix and Suffix","id":"/Basic-Types/Strings/#The-Lean-Language-Reference--Basic-Types--Strings--API-Reference--Raw-Substrings--Prefix-and-Suffix"},"/Error-Explanations/About___--inductiveParamMismatch/#The-Lean-Language-Reference--Error-Explanations--About___--inductiveParamMismatch--Examples":{"contents":"Vector Length Index as a Parameterinductive Vec (α : Type) (n : Nat) : Type where\n  | nil  : Vec α 0\n  | cons : α → Vec α n → Vec α (n + 1)\nMismatched inductive type parameter in\n  Vec α 0\nThe provided argument\n  0\nis not definitionally equal to the expected parameter\n  n\n\nNote: The value of parameter `n` must be fixed throughout the inductive declaration. Consider making this parameter an index if it must vary.\ninductive Vec (α : Type) : Nat → Type where\n  | nil  : Vec α 0\n  | cons : α → Vec α n → Vec α (n + 1)\nThe length argument n of the Vec type constructor is declared as a parameter, but other values\nfor this argument appear in the nil and cons constructors (namely, 0 and n + 1). An error\ntherefore appears at the first occurrence of such an argument. To correct this, n cannot be a\nparameter of the inductive declaration and must instead be an index, as in the corrected example. On\nthe other hand, α remains unchanged throughout all occurrences of Vec in the declaration and so\nis a valid parameter.\n\n","context":"Lean Reference\u0009Error Explanations\u0009About:  inductiveParamMismatch","header":"Examples","id":"/Error-Explanations/About___--inductiveParamMismatch/#The-Lean-Language-Reference--Error-Explanations--About___--inductiveParamMismatch--Examples"},"/Notations-and-Macros/Defining-New-Syntax/#source-info":{"contents":"Atoms, identifiers, and nodes optionally contain source information that tracks their correspondence with the original file.\nThe parser saves source information for all tokens, but not for nodes; position information for parsed nodes is reconstructed from their first and last tokens.\nNot all Syntax data results from the parser: it may be the result of macro expansion, in which case it typically contains a mix of generated and parsed syntax, or it may be the result of delaborating an internal term to display it to a user.\nIn these use cases, nodes may themselves contain source information.\n\nSource information comes in two varieties:\n\n Original\n\nOriginal source information comes from the parser.\n  In addition to the original source location, it also contains leading and trailing whitespace that was skipped by the parser, which allows the original string to be reconstructed.\n  This whitespace is saved as offsets into the string representation of the original source code (that is, as Substring) to avoid having to allocate copies of substrings.\n\n Synthetic\n\nSynthetic source information comes from metaprograms (including macros) or from Lean's internals.\n  Because there is no original string to be reconstructed, it does not save leading and trailing whitespace.\n  Synthetic source positions are used to provide accurate feedback even when terms have been automatically transformed, as well as to track the correspondence between elaborated expressions and their presentation in Lean's output.\n  A synthetic position may be marked canonical, in which case some operations that would ordinarily ignore synthetic positions will treat it as if it were not.\n\n\n\nSource information that relates syntax to the context that it came from.The primary purpose of SourceInfo is to relate the output of the parser and the macro expander to\nthe original source file. When produced by the parser, Syntax.node does not carry source info; the\nparser associates it only with atoms and identifiers. If a Syntax.node is introduced by a\nquotation, then it has synthetic source info that both associates it with an original reference\nposition and indicates that the original atoms in it may not originate from the Lean file under\nelaboration.Source info is also used to relate Lean's output to the internal data that it represents; this is\nthe basis for many interactive features. When used this way, it can occur on Syntax.node as well.A token produced by the parser from original input that includes both leading and trailing\nwhitespace as well as position information.The leading whitespace is inferred after parsing by Syntax.updateLeading. This is because the\n“preceding token” is not well-defined during parsing, especially in the presence of backtracking.Synthetic syntax is syntax that was produced by a metaprogram or by Lean itself (e.g. by a\nquotation). Synthetic syntax is annotated with a source span from the original syntax, which\nrelates it to the source file.The delaborator uses this constructor to store an encoded indicator of which core language\nexpression gave rise to the syntax.The canonical flag on synthetic syntax is enabled for syntax that is not literally part of the\noriginal input syntax but should be treated “as if” the user really wrote it for the purpose of\nhovers and error messages. This is usually used on identifiers in order to connect the binding\nsite to the user's original syntax even if the name of the identifier changes during expansion, as\nwell as on tokens that should receive targeted messages.Generally speaking, a macro expansion should only use a given piece of input syntax in a single\ncanonical token. An exception to this rule is when the same identifier is used to declare two\nbinders, as in the macro expansion for dependent if:`(if $h : $cond then $t else $e) ~>\n`(dite $cond (fun $h => $t) (fun $h => $t))\nIn these cases, if the user hovers over h they will see information about both binding sites.A synthesized token without position information.\n\n","context":"Lean Reference\u0009Notations and Macros\u0009Defining New Syntax","header":"22.4.5. Source Positions","id":"/Notations-and-Macros/Defining-New-Syntax/#source-info"},"/releases/v4.27.0/#The-Lean-Language-Reference--Release-Notes--Lean-4___27___0-_LPAR_2026-01-24_RPAR_--Tactics":{"contents":"* #11226 finally removes the old grind framework SearchM. It has been\nreplaced with the new Action framework.* #11244 fixes minor issues in grind. In preparation for adding grind -revert.* #11247 fixes an issue in the grind preprocessor. simp may introduce\nassigned (universe) metavariables (e.g., when performing\nzeta-reduction).* #11248 implements the option revert, which is set to false by\ndefault. To recover the old grind behavior, you should use grind +revert. Previously, grind used the RevSimpIntro idiom, i.e., it\nwould revert all hypotheses and then re-introduce them while simplifying\nand applying eager cases. This idiom created several problems:* Users reported that grind would include unnecessary parameters. See\nhere.* Unnecessary section variables were also being introduced. See the new\ntest contributed by Sebastian Graf.* Finally, it prevented us from supporting arbitrary parameters as we do\nin simp. In simp, I implemented a mechanism that simulates local\nuniverse-polymorphic theorems, but this approach could not be used in\ngrind because there is no mechanism for reverting (and re-introducing)\nlocal universe-polymorphic theorems. Adding such a mechanism would\nrequire substantial work: I would need to modify the local context\nobject. I considered maintaining a substitution from the original\nvariables to the new ones, but this is also tricky, because the mapping\nwould have to be stored in the grind goal objects, and it is not just\na simple mapping. After reverting everything, I would need to keep a\nsequence of original variables that must be added to the mapping as we\nre-introduce them, but eager case splits complicate this quite a bit.\nThe whole approach felt overly messy.* #11265 marks the automatically generated sizeOf theorems as grind\ntheorems.* #11268 implements support for arbitrary grind parameters. The feature\nis similar to the one available in simp, where a proof term is treated\nas a local universe-polymorphic lemma. This feature relies on grind -revert (see #11248). For example, users can now write:def snd (p : α × β) : β := p.2\ntheorem snd_eq (a : α) (b : β) : snd (a, b) = b := rfl\n\n* #11273 fixes a bug during proof construction in grind.* #11295 fixes a bug in the propagation rules for ite and dite used\nin grind. The bug prevented equalities from being propagated to the\nsatellite solvers. Here is an example affected by this issue.* #11315 fixes an issue affecting grind -revert. In this mode, assigned\nmetavariables in hypotheses were not being instantiated. This issue was\naffecting two files in Mathlib.* #11318 fixes a local declaration internalization in grind that was\nexposed when using grind -revert. This bug was affecting a grind\nproof in Mathlib.* #11319 improves the support for Fin n in grind when n is not a\nnumeral.* #11323 introduces a new grind option, funCC (enabled by default),\nwhich extends congruence closure to function-valued equalities. When\nfunCC is enabled, grind tracks equalities of partially applied\nfunctions, allowing reasoning steps such as:a : Nat → Nat\nf : (Nat → Nat) → (Nat → Nat)\nh : f a = a\n⊢ (f a) m = a m\n\n* #11326 ensures that users can provide grind proof parameters whose\ntypes are not forall-quantified. Examples:opaque f : Nat → Nat\naxiom le_f (a : Nat) : a ≤ f a\n\n* #11330 renames the cutsat tactic to lia for better alignment with\nstandard terminology in the theorem proving community.* #11331 adds support for the LawfulOfScientific class in grind.\nExamples:open Lean Grind Std\nvariable [LE α] [LT α] [LawfulOrderLT α] [Field α] [OfScientific α]\n         [LawfulOfScientific α] [IsLinearOrder α] [OrderedRing α]\nexample : (2 / 3 : α) ≤ (0.67 : α) := by  grind\nexample : (1.2 : α) ≤ (1.21 : α) := by grind\nexample : (2 / 3 : α) ≤ (67 / 100 : α) := by grind\nexample : (1.2345 : α) ≤ (1.2346 : α) := by grind\nexample : (2.3 : α) ≤ (4.5 : α) := by grind\nexample : (2.3 : α) ≤ (5/2 : α) := by grind\n* #11332 adds a grind_annotated \"YYYY-MM-DD\" command that marks files\nas manually annotated for grind.* #11334 adds an explicit normalization layer for ring constraints in the\ngrind linarith module. For example, it will be used to clean up\ndenominators when the ring is a field.* #11335 enables the syntax use [ns Foo] and instantiate only [ns Foo] inside a grind tactic block, and has the effect of activating\nall grind patterns scoped to that namespace. We can use this to\nimplement specialized tactics using grind, but only controlled subsets\nof theorems.* #11348 activates the grind_annotated command in\nInit.Data.List.Lemmas by removing the TODO comment and uncommenting\nthe command.* #11350 implements a helper simproc for grind. It is part of the\ninfrastructure used to cleanup denominators in grind linarith.* #11365 enables parallelism in try?. Currently, we replace the\nattempt_all stages (there are two, one for builtin tactics including\ngrind and simp_all, and a second one for all user extensions) with\nparallel versions. We do not (yet?) change the behaviour of first\nbased stages.* #11373 makes the library suggestions extension state available when\nimporting from module files.* #11375 adds support for cleaning up denominators in grind linarith\nwhen the type is a Field.* #11391 implements new kinds of constraints for the grind_pattern\ncommand. These constraints allow users to control theorem instantiation\nin grind.\nIt requires a manual update-stage0 because the change affects the\n.olean format, and the PR fails without it.* #11396 changes set_library_suggestions to create an auxiliary\ndefinition marked with @[library_suggestions], rather than storing\nSyntax directly in the environment extension. This enables better\npersistence and consistency of library suggestions across modules.* #11405 implements the following grind_pattern constraints:grind_pattern fax => f x  where\n  depth x < 2\n\n* #11409 implements support for the grind_pattern constraints\nis_value and is_strict_value.* #11410 fixes a kernel type mismatch error in grind's denominator\ncleanup feature. When generating proofs involving inverse numerals (like\n2⁻¹), the proof context is compacted to only include variables\nactually used. This involves renaming variable indices - e.g., if\noriginal indices were {0: r, 1: 2⁻¹} and only 2⁻¹ is used, it gets\nrenamed to index 0.* #11412 fixes an issue where grind would fail after multiple\nnorm_cast\ncalls with the error \"unexpected metadata found during internalization\".* #11428 implements support for guards in grind_pattern. The new\nfeature provides additional control over theorem instantiation. For\nexample, consider the following monotonicity theorem:opaque f : Nat → Nat\ntheorem fMono : x ≤ y → f x ≤ f y := ...\n* #11429 documents the grind_pattern command for manually selecting\ntheorem instantiation patterns, including multi-patterns and the\nconstraint system (=/=, =?=, size, depth, is_ground,\nis_value, is_strict_value, gen, max_insts, guard, check).* #11462 adds solve_by_elim as a fallback in the try? tactic's simple\ntactics. When rfl and assumption both fail but solve_by_elim\nsucceeds (e.g., for goals requiring hypothesis chaining or\nbacktracking), try? will now suggest solve_by_elim.* #11464 improves the error message when no library suggestions engine is\nregistered to recommend importing Lean.LibrarySuggestions.Default for\nthe built-in engine.* #11466 removes the \"first pass\" behavior where exact? and apply?\nwould try solve_by_elim on the original goal before doing library\nsearch. This simplifies the librarySearch API and focuses these\ntactics on their primary purpose: finding library lemmas.* #11468 adds +suggestions support to solve_by_elim, following the\npattern established by grind +suggestions and simp_all +suggestions.* #11469 adds +grind and +try? options to exact? and apply?\ntactics.* #11471 fixes an incorrect reducibility setting when using grind\ninteractive mode.* #11480 adds the grind option reducible (default: true). When\nenabled, definitional equality tests expand only declarations marked as\n@[reducible].\nUse grind -reducible to allow expansion of non-reducible declarations\nduring definitional equality tests.\nThis option affects only definitional equality; the canonicalizer and\ntheorem pattern internalization always unfold reducible declarations\nregardless of this setting.* #11481 fixes a bug in grind?. The suggestion using the grind\ninteractive mode was dropping the configuration options provided by the\nuser. In the following account, the third suggestion was dropping the\n-reducible option.* #11484 fixes a bug in the grind pattern validation. The bug affected\ntype classes that were propositions.* #11487 adds a heterogeneous version of the constructor injectivity\ntheorems. These theorems are useful for indexed families, and will be\nused in grind.* #11491 implements heterogeneous constructor injectivity in grind.* #11494 re-enables star-indexed lemmas as a fallback for exact? and\napply?.* #11519 marks Nat power and divisibility theorems for grind. We use\nthe new grind_pattern constraints to control theorem instantiation.\nExamples:example {x m n : Nat} (h : x = 4 ^ (m + 1) * n) : x % 4 = 0 := by\n  grind\n\n* #11520 implements the constraint not_value x in the grind_pattern\ncommand. It is the negation of the constraint is_value.* #11522 implements grind propagators for Nat operators that have a\nsimproc associated with them, but do not have any theory solver support.\nExamples:example (a b : Nat) : a = 3 → b = 6 → a &&& b = 2 := by grind\nexample (a b : Nat) : a = 3 → b = 6 → a ||| b = 7 := by grind\nexample (a b : Nat) : a = 3 → b = 6 → a ^^^ b = 5 := by grind\nexample (a b : Nat) : a = 3 → b = 6 → a <<< b = 192 := by grind\nexample (a b : Nat) : a = 1135 → b = 6 → a >>> b = 17 := by grind\n* #11547 ensures the auxiliary definitions created by\nregister_try?_tactic are internal implementation details that should\nnot be visible to user-facing linters.* #11556 adds a +all option to exact? and apply? that collects all\nsuccessful lemmas instead of stopping at the first complete solution.* #11573 fixes grind rejecting dot notation terms, mistaking them for\nlocal hypotheses.* #11579 ensures that ground theorems are properly handled as grind\nparameters. Additionally, grind [(thm)] and grind [thm] should be\nhandled the same way.* #11580 adds a missing Nat.cast missing normalization rule for\ngrind. Example:example (n : Nat) : Nat.cast n = n := by\n  grind\n* #11589 improves indexing for grind patterns. We now include symbols\noccurring in nested ground patterns. This important to minimize the\nnumber of activated E-match theorems.* #11593 fixes an issue where grind did not display deprecation\nwarnings when deprecated lemmas were used in its argument list.* #11594 fixes grind? to include term parameters (like [show P by tac]) in its suggestions. Previously, these were being dropped because\nterm arguments are stored in extraFacts and not tracked via E-matching\nlike named lemmas.* #11604 fixes how theorems without parameters are handled in grind.* #11605 fixes a bug in the internalizer of a^p terms in grind linarith.* #11609 improves the case-split heuristics in grind. In this PR, we do\nnot increment the number of case splits in the first case. The idea is\nto leverage non-chronological backtracking: if the first case is solved\nusing a proof that doesn't depend on the case hypothesis, we backtrack\nand close the original goal directly. In this scenario, the case-split\nwas \"free\", it didn't contribute to the proof. By not counting it, we\nallow deeper exploration when case-splits turn out to be irrelevant.\nThe new heuristic addresses the second example in #11545* #11613 ensures we apply the ring normalizer to equalities being\npropagated from the grind core module to grind lia. It also ensures\nwe use the safe/managed polynomial functions when normalizing.* #11615 adds a normalization rule for Int.subNatNat to grind.* #11628 adds a few * normalization rules for Semirings to grind.* #11629 adds a missing condition in the pattern normalization code used\nin grind. It should ignore support ground terms.* #11635 ensures the pattern normalizer used in grind does violate\nassumptions made by the gadgets Grind.genPattern and\nGrind.getHEqPattern.* #11638 fixes bitvector literal internalization in grind. The fix\nensures theorems indexed by BitVec.ofNat are properly activated.* #11639 adds support for BitVec.ofNat in grind ring. Example:example (x : BitVec 8) : (x - 16#8)*(x + 272#8) = x^2 := by\n  grind\n* #11640 adds support for BitVec.ofNat in grind lia. Example:example (x y : BitVec 8) : y < 254#8 → x > 2#8 + y → x > 1#8 + y := by\n  grind\n* #11653 adds propagation rules corresponding to the Semiring\nnormalization rules introduced in #11628. The new rules apply only to\nnon-commutative semirings, since support for them in grind is limited.\nThe normalization rules introduced unexpected behavior in Mathlib\nbecause they neutralize parameters such as one_mul: any theorem\ninstance associated with such a parameter is reduced to True by the\nnormalizer.* #11656 adds support for Int.sign, Int.fdiv, Int.tdiv, Int.fmod,\nInt.tmod, and Int.bmod to grind. These operations are just\npreprocessed away. We assume that they are not very common in practice.\nExamples:example {x y : Int} : y = 0 → (x.fdiv y) = 0 := by grind\nexample {x y : Int} : y = 0 → (x.tdiv y) = 0 := by grind\nexample {x y : Int} : y = 0 → (x.fmod y) = x := by grind\nexample {x y : Int} : y = 1 → (x.fdiv (2 - y)) = x := by grind\nexample {x : Int} : x > 0 → x.sign = 1 := by grind\nexample {x : Int} : x < 0 → x.sign = -1 := by grind\nexample {x y : Int} : x.sign = 0 → x*y = 0 := by grind\n* #11658 fixes a bug in the internalization of parametric literals in\ngrind. That is, literals whose type is BitVec _ or Fin _.* #11659 adds MessageData.withNamingContext when generating pattern\nsuggestions at @[grind]. It fixes another issue reported during\nItaLean.* #11660 fixes another theorem activation issue in grind.* #11663 fixes the grind pattern validator. It covers the case where an\ninstance is not tagged with the implicit instance binder. This happens\nin declarations such asZeroMemClass.zero_mem {S : Type} {M : outParam Type} {inst1 : Zero M} {inst2 : SetLike S M}\n  [self : @ZeroMemClass S M inst1 inst2] (s : S) : 0 ∈ s\n\n\n","context":"Lean Reference\u0009Release Notes\u0009Lean 4.27.0 (2026-01-24)","header":"Tactics","id":"/releases/v4.27.0/#The-Lean-Language-Reference--Release-Notes--Lean-4___27___0-_LPAR_2026-01-24_RPAR_--Tactics"}});