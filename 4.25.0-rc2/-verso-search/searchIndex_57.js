window.docContents[57].resolve({"/Basic-Types/Arrays/#array-syntax":{"contents":"Array literals allow arrays to be written directly in code.\nThey may be used in expression or pattern contexts.\n\nArray LiteralsArray literals begin with #[ and contain a comma-separated sequence of terms, terminating with ].\n\nArray LiteralsArray literals may be used as expressions or as patterns.def oneTwoThree : Array Nat := #[1, 2, 3]\n\n#eval\n  match oneTwoThree with\n  | #[x, y, z] => some ((x + z) / y)\n  | _ => none\n\n\nAdditionally, sub-arrays may be extracted using the following syntax:\n\nSub-ArraysA start index followed by a colon constructs a sub-array that contains the values from the start index onwards (inclusive):Providing start and end indices  constructs a sub-array that contains the values from the start index (inclusive) to the end index (exclusive):\n\nSub-Array SyntaxThe array ten contains the first ten natural numbers.def ten : Array Nat :=\n  .range 10\nA sub-array that represents the second half of ten can be constructed using the sub-array syntax:#eval ten[5:]\n#[5, 6, 7, 8, 9].toSubarray\nSimilarly, sub-array that contains two through five can be constructed by providing a stopping point:#eval ten[2:6]\n#[2, 3, 4, 5].toSubarray\nBecause sub-arrays merely store the start and end indices of interest in the underlying array, the array itself can be recovered:#eval ten[2:6].array == ten\ntrue\n\n\n","context":"Lean Reference\u0009Basic Types\u0009Arrays","header":"19.16.3. Syntax","id":"/Basic-Types/Arrays/#array-syntax"},"/Basic-Types/Linked-Lists/#The-Lean-Language-Reference--Basic-Types--Linked-Lists--API-Reference--Predicates-and-Relations":{"contents":"The first list is a prefix of the second.IsPrefix l₁ l₂, written l₁ <+: l₂, means that there exists some t : List α such that l₂ has\nthe form l₁ ++ t.The function List.isPrefixOf is a Boolean equivalent.Conventions for notations in identifiers:* The recommended spelling of <+: in identifiers is prefix (not isPrefix).\n\nList PrefixThe first list is a prefix of the second.IsPrefix l₁ l₂, written l₁ <+: l₂, means that there exists some t : List α such that l₂ has\nthe form l₁ ++ t.The function List.isPrefixOf is a Boolean equivalent.Conventions for notations in identifiers:* The recommended spelling of <+: in identifiers is prefix (not isPrefix).\n\nThe first list is a suffix of the second.IsSuffix l₁ l₂, written l₁ <:+ l₂, means that there exists some t : List α such that l₂ has\nthe form t ++ l₁.The function List.isSuffixOf is a Boolean equivalent.Conventions for notations in identifiers:* The recommended spelling of <:+ in identifiers is suffix (not isSuffix).\n\nList SuffixThe first list is a suffix of the second.IsSuffix l₁ l₂, written l₁ <:+ l₂, means that there exists some t : List α such that l₂ has\nthe form t ++ l₁.The function List.isSuffixOf is a Boolean equivalent.Conventions for notations in identifiers:* The recommended spelling of <:+ in identifiers is suffix (not isSuffix).\n\nThe first list is a contiguous sub-list of the second list. Typically written with the <:+:\noperator.In other words, l₁ <:+: l₂ means that there exist lists s : List α and t : List α such that\nl₂ has the form s ++ l₁ ++ t.Conventions for notations in identifiers:* The recommended spelling of <:+: in identifiers is infix (not isInfix).\n\nList InfixThe first list is a contiguous sub-list of the second list. Typically written with the <:+:\noperator.In other words, l₁ <:+: l₂ means that there exist lists s : List α and t : List α such that\nl₂ has the form s ++ l₁ ++ t.Conventions for notations in identifiers:* The recommended spelling of <:+: in identifiers is infix (not isInfix).\n\nThe first list is a non-contiguous sub-list of the second list. Typically written with the <+\noperator.In other words, l₁ <+ l₂ means that l₁ can be transformed into l₂ by repeatedly inserting new\nelements.The base case: [] is a sublist of []If l₁ is a subsequence of l₂, then it is also a subsequence of a :: l₂.If l₁ is a subsequence of l₂, then a :: l₁ is a subsequence of a :: l₂.\n\nSublistsThe first list is a non-contiguous sub-list of the second list. Typically written with the <+\noperator.In other words, l₁ <+ l₂ means that l₁ can be transformed into l₂ by repeatedly inserting new\nelements.This syntax is only available when the List namespace is opened.\n\nTwo lists are permutations of each other if they contain the same elements, each occurring the same\nnumber of times but not necessarily in the same order.One list can be proven to be a permutation of another by showing how to transform one into the other\nby repeatedly swapping adjacent elements.List.isPerm is a Boolean equivalent of this relation.The empty list is a permutation of the empty list: [] ~ [].If one list is a permutation of the other, adding the same element as the head of each yields\nlists that are permutations of each other: l₁ ~ l₂ → x::l₁ ~ x::l₂.If two lists are identical except for having their first two elements swapped, then they are\npermutations of each other: x::y::l ~ y::x::l.Permutation is transitive: l₁ ~ l₂ → l₂ ~ l₃ → l₁ ~ l₃.\n\nList PermutationTwo lists are permutations of each other if they contain the same elements, each occurring the same\nnumber of times but not necessarily in the same order.One list can be proven to be a permutation of another by showing how to transform one into the other\nby repeatedly swapping adjacent elements.List.isPerm is a Boolean equivalent of this relation.This syntax is only available when the List namespace is opened.\n\nEach element of a list is related to all later elements of the list by R.Pairwise R l means that all the elements of l with earlier indexes are R-related to all the\nelements with later indexes.For example, Pairwise (· ≠ ·) l asserts that l has no duplicates, and Pairwise (· < ·) l\nasserts that l is (strictly) sorted.Examples:* Pairwise (· < ·) [1, 2, 3] ↔ (1 < 2 ∧ 1 < 3) ∧ 2 < 3* Pairwise (· = ·) [1, 2, 3] = False* Pairwise (· ≠ ·) [1, 2, 3] = TrueAll elements of the empty list are vacuously pairwise related.A nonempty list is pairwise related with R if the head is related to every element of the tail\nand the tail is itself pairwise related.That is, a :: l is Pairwise R if:* R relates a to every element of l* l is Pairwise R.\n\nThe list has no duplicates: it contains every element at most once.It is defined as Pairwise (· ≠ ·): each element is unequal to all other elements.\n\nLexicographic ordering for lists with respect to an ordering of elements.as is lexicographically smaller than bs if* as is empty and bs is non-empty, or* both as and bs are non-empty, and the head of as is less than the head of bs according to\nr, or* both as and bs are non-empty, their heads are equal, and the tail of as is less than the\ntail of bs.[] is the smallest element in the lexicographic order.If the head of the first list is smaller than the head of the second, then the first list is\nlexicographically smaller than the second list.If two lists have the same head, then their tails determine their lexicographic order. If the tail\nof the first list is lexicographically smaller than the tail of the second list, then the entire\nfirst list is lexicographically smaller than the entire second list.\n\nList membership, typically accessed via the ∈ operator.a ∈ l means that a is an element of the list l. Elements are compared according to Lean's\nlogical equality.The related function List.elem is a Boolean membership test that uses a BEq α instance.Examples:* a ∈ [x, y, z] ↔ a = x ∨ a = y ∨ a = zThe head of a list is a member: a ∈ a :: as.A member of the tail of a list is a member of the list: a ∈ l → a ∈ b :: l.\n\n","context":"Lean Reference\u0009Basic Types\u0009Linked Lists\u0009API Reference","header":"19.15.3.1. Predicates and Relations","id":"/Basic-Types/Linked-Lists/#The-Lean-Language-Reference--Basic-Types--Linked-Lists--API-Reference--Predicates-and-Relations"},"/Build-Tools-and-Distribution/Lake/#The-Lean-Language-Reference--Build-Tools-and-Distribution--Lake--Script-API-Reference--Accessing-the-Environment--Elan-Install-Helpers":{"contents":"Get the detected Elan installation (if one).\n\nGet the root directory of the detected Elan installation (i.e., ELAN_HOME).\n\nGet the path of the elan binary in the detected Elan installation.\n\n","context":"Lean Reference\u0009Build Tools\u0009Lake\u0009Script API Reference\u0009Accessing the Environment","header":"22.1.4.1.2. Elan Install Helpers","id":"/Build-Tools-and-Distribution/Lake/#The-Lean-Language-Reference--Build-Tools-and-Distribution--Lake--Script-API-Reference--Accessing-the-Environment--Elan-Install-Helpers"},"/Build-Tools-and-Distribution/Lake/#The-Lean-Language-Reference--Build-Tools-and-Distribution--Lake--Script-API-Reference--Accessing-the-Environment--Lean-Install-Helpers":{"contents":"Get the detected Lean installation.\n\nGet the root directory of the detected Lean installation.\n\nGet the Lean source directory of the detected Lean installation.\n\nGet the Lean library directory of the detected Lean installation.\n\nGet the C include directory of the detected Lean installation.\n\nGet the system library directory of the detected Lean installation.\n\nGet the path of the lean binary in the detected Lean installation.\n\nGet the path of the leanc binary in the detected Lean installation.\n\nGet the path of the libleanshared library in the detected Lean installation.\n\nGet the path of the ar binary in the detected Lean installation.\n\nGet the path of C compiler in the detected Lean installation.\n\nGet the optional LEAN_CC compiler override of the detected Lean installation.\n\n","context":"Lean Reference\u0009Build Tools\u0009Lake\u0009Script API Reference\u0009Accessing the Environment","header":"22.1.4.1.3. Lean Install Helpers","id":"/Build-Tools-and-Distribution/Lake/#The-Lean-Language-Reference--Build-Tools-and-Distribution--Lake--Script-API-Reference--Accessing-the-Environment--Lean-Install-Helpers"},"/Error-Explanations/lean___unknownIdentifier/#The-Lean-Language-Reference--Error-Explanations--lean___unknownIdentifier":{"contents":"\n\nThis error means that Lean was unable to find a variable or constant matching the given name. More\nprecisely, this means that the name could not be resolved, as described in the manual section on\nIdentifiers: no interpretation of the input as\nthe name of a local or section variable (if applicable), a previously declared global constant, or a\nprojection of either of the preceding was valid. (\"If applicable\" refers to the fact that in some\ncases—e.g., the #print command's argument—names are resolved only to global constants.)\n\nNote that this error message will display only one possible resolution of the identifier, but the\npresence of this error indicates failures for all possible names to which it might refer. For\nexample, if the identifier x is entered with the namespaces A and B are open, the error\nmessage \"Unknown identifier `x`\" indicates that none of x, A.x, or B.x could be found (or\nthat A.x or B.x, if either exists, is a protected declaration).\n\nCommon causes of this error include forgetting to import the module in which a constant is defined,\nomitting a constant's namespace when that namespace is not open, or attempting to refer to a local\nvariable that is not in scope.\n\nTo help resolve some of these common issues, this error message is accompanied by a code action that\nsuggests constant names similar to the one provided. These include constants in the environment as\nwell as those that can be imported from other modules. Note that these suggestions are available\nonly through supported code editors' built-in code action mechanisms and not as a hint in the error\nmessage itself.\n\n\n\n","context":"Lean Reference\u0009Error Explanations","header":"lean.unknownIdentifier","id":"/Error-Explanations/lean___unknownIdentifier/#The-Lean-Language-Reference--Error-Explanations--lean___unknownIdentifier"},"/Terms/Numeric-Literals/#The-Lean-Language-Reference--Terms--Numeric-Literals--Scientific-Numbers":{"contents":"Scientific number literals consist of a sequence of decimal digits followed (without intervening whitespace) by an optional decimal part (a period followed by zero or more decimal digits) and an optional exponent part (the letter e followed by an optional + or - and then followed by one or more decimal digits).\nScientific numbers are overloaded via the OfScientific type class.\n\nFor decimal and scientific numbers (e.g., 1.23, 3.12e10).\nExamples:* 1.23 is syntax for OfScientific.ofScientific (nat_lit 123) true (nat_lit 2)* 121e100 is syntax for OfScientific.ofScientific (nat_lit 121) false (nat_lit 100)Note the use of nat_lit; there is no wrapping OfNat.ofNat in the resulting term.Produces a value from the given mantissa, exponent sign, and decimal exponent. For the exponent\nsign, true indicates a negative exponent.Examples:* 1.23 is syntax for OfScientific.ofScientific (nat_lit 123) true (nat_lit 2)* 121e100 is syntax for OfScientific.ofScientific (nat_lit 121) false (nat_lit 100)Note the use of nat_lit; there is no wrapping OfNat.ofNat in the resulting term.\n\nThere are an OfScientific instances for Float and Float32, but no separate floating-point literals.\n\n","context":"Lean Reference\u0009Terms\u0009Numeric Literals","header":"10.5.2. Scientific Numbers","id":"/Terms/Numeric-Literals/#The-Lean-Language-Reference--Terms--Numeric-Literals--Scientific-Numbers"},"/The--grind--tactic/E___matching/#e-matching-patterns":{"contents":"The E-matching index is a table of patterns.\nWhen a term matches one of the patterns in the table, grind attempts to instantiate and apply the corresponding theorem, giving rise to further facts and equalities.\nSelecting appropriate patterns is an important part of using grind effectively: if the patterns are too restrictive, then useful theorems may not be applied; if they are too general, performance may suffer.\n\nE-matching PatternsConsider the following functions and theorems:def f (a : Nat) : Nat :=\n  a + 1\n\ndef g (a : Nat) : Nat :=\n  a - 1\n\n@[grind =]\ntheorem gf (x : Nat) : g (f x) = x := by\n  simp [f, g]\nThe theorem gf asserts that g (f x) = x for all natural numbers x.\nThe attribute grind = instructs grind to use the left-hand side of the equation, g (f x), as a pattern for heuristic instantiation via E-matching.This proof goal does not include an instance of g (f x), but grind is nonetheless able to solve it:example {a b} (h : f b = a) : g a = b := by\n  grind\nAlthough g a is not an instance of the pattern g (f x), it becomes one modulo the equation f b = a.\nBy substituting a with f b in g a, we obtain the term g (f b), which matches the pattern g (f x) with the assignment x := b.\nThus, the theorem gf is instantiated with x := b, and the new equality g (f b) = b is asserted.\ngrind then uses congruence closure to derive the implied equality g a = g (f b) and completes the proof.\n\nThe grind_pattern command can be used to manually select an E-matching pattern for a theorem.\nEnabling the option trace.grind.ematch.instance causes grind print a trace message for each theorem instance it generates, which can be helpful when determining E-matching patterns.\n\nE-matching Pattern SelectionAssociates a theorem with one or more patterns.\nWhen multiple patterns are provided in a single grind_pattern command, all of them must match a term before grind will attempt to instantiate the theorem.\n\nSelecting PatternsThe grind = attribute uses the left side of the equality as the E-matching pattern for gf:def f (a : Nat) : Nat :=\n  a + 1\n\ndef g (a : Nat) : Nat :=\n  a - 1\n\n@[grind =]\ntheorem gf (x : Nat) : g (f x) = x := by\n  simp [f, g]\nFor example, the pattern g (f x) is too restrictive in the following case:\nthe theorem gf will not be instantiated because the goal does not even\ncontain the function symbol g.In this example, grind fails because the pattern is too restrictive: the goal does not contain the function symbol g.example (h₁ : f b = a) (h₂ : f c = a) : b = c := by\n  grind\n`grind` failed\ncase grind\nb a c : Nat\nh₁ : f b = a\nh₂ : f c = a\nh : ¬b = c\n⊢ False\n[grind] Goal diagnostics\n  [facts] Asserted facts\n  [eqc] False propositions\n    [prop] b = c\n  [eqc] Equivalence classes\n    [eqc] {a, f b, f c}\nUsing just f x as the pattern allows grind to solve the goal automatically:grind_pattern gf => f x\n\nexample {a b c} (h₁ : f b = a) (h₂ : f c = a) : b = c := by\n  grind\nEnabling trace.grind.ematch.instance makes it possible to see the equalities found by E-matching:example (h₁ : f b = a) (h₂ : f c = a) : b = c := by\n  set_option trace.grind.ematch.instance true in\n  grind\n[grind.ematch.instance] gf: g (f c) = c\n[grind.ematch.instance] gf: g (f b) = b\nAfter E-matching, the proof succeeds because congruence closure equates g (f c) with g (f b), because both f b and f c are equal to a.\nThus, b and c must be in the same equivalence class.\n\nWhen multiple patterns are specified together, all of them must match in the current context before grind attempts to instantiate the theorem.\nThis is referred to as a multi-pattern.\nThis is useful for lemmas such as transitivity rules, where multiple premises must be simultaneously present for the rule to apply.\nA single theorem may be associated with multiple separate patterns by using multiple invocations of grind_pattern or the @[grind _=_] attribute.\nIf any of these separate patterns match, the theorem will be instantiated.\n\nMulti-PatternsR is a transitive binary relation over Int:opaque R : Int → Int → Prop\naxiom Rtrans {x y z : Int} : R x y → R y z → R x z\nTo use the fact that R is transitive, grind must already be able to satisfy both premises.\nThis is represented using a multi-pattern:grind_pattern Rtrans => R x y, R y z\n\nexample {a b c d} : R a b → R b c → R c d → R a d := by\n  grind\nThe multi-pattern R x y, R y z instructs grind to instantiate Rtrans only when both R x y and R y z are available in the context.\nIn the example, grind applies Rtrans to derive R a c from R a b and R b c, and can then repeat the same reasoning to deduce R a d from R a c and R c d.\n\nThe grind attribute automatically generates an E-matching pattern or multi-pattern using a heuristic, instead of using grind_pattern to explicitly specify a pattern.\nIt includes a number of variants that select different heuristics.\nThe grind? attribute displays an info message showing the pattern which was selected—this is very helpful for debugging!\n\nPatterns are subexpressions of theorem statements.\nA subexpression is indexable if it has an indexable constant as its head, and it is said to cover one of the theorem's arguments if it fixes the argument's value.\nIndexable constants are all constants other than Eq, HEq, Iff, And, Or, and Not.\nThe set of arguments that are covered by a pattern or multi-pattern is referred to as its coverage.\nSome constants are lower priority than others; in particular, the arithmetic operators HAdd.hAdd, HSub.hSub, HMul.hMul, Dvd.dvd, HDiv.hDiv, and HMod.hMod have low priority.\nAn indexable subexpression is minimal if there is no smaller indexable subexpression whose head constant has at least as high priority.\n\nGrind PatternsThe grind attribute automatically generates an E-matching pattern for a theorem, using a strategy determined by the provided modifier.\nIf no modifier is provided, then grind suggests suitable modifiers, displaying the resulting patterns.The grind! attribute automatically generates an E-matching pattern for a theorem, using a strategy determined by the provided modifier.\nIt additionally enforces the condition that the selected pattern(s) should be minimal indexable subexpressions.The grind? displays the pattern that was generated.The grind!? attribute is equivalent to grind!, except it displays the resulting pattern for inspection.Without any modifier, @[grind] traverses the conclusion and then the hypotheses from left to right, adding patterns as they increase the coverage, stopping when all arguments are covered.\nThis default strategy can be explicitly requested using the . modifier.\nIn addition to using the default strategy, the attribute checks which other strategies could be applied, and displays all of the resulting patterns.\n\n\n\nDefault PatternThe . modifier instructs grind to select a multi-pattern by traversing the conclusion of the\ntheorem, and then the hypotheses from eft to right. We say this is the default modifier.\nEach time it encounters a subexpression which covers an argument which was not\npreviously covered, it adds that subexpression as a pattern, until all arguments have been covered.\nIf grind! is used, then only minimal indexable subexpressions are considered.\n\nEquality RewritesThe = modifier instructs grind to check that the conclusion of the theorem is an equality,\nand then uses the left-hand side of the equality as a pattern. This may fail if not all of the arguments appear\nin the left-hand side.\n\nBackward Equality RewritesThe =_ modifier instructs grind to check that the conclusion of the theorem is an equality,\nand then uses the right-hand side of the equality as a pattern. This may fail if not all of the arguments appear\nin the right-hand side.\n\nBidirectional Equality RewritesThe _=_ modifier acts like a macro which expands to = and =_.  It adds two patterns,\nallowing the equality theorem to trigger in either direction.\n\nForward ReasoningThe → modifier instructs grind to select a multi-pattern from the hypotheses of the theorem.\nIn other words, grind will use the theorem for forwards reasoning.\nTo generate a pattern, it traverses the hypotheses of the theorem from left to right.\nEach time it encounters a subexpression which covers an argument which was not\npreviously covered, it adds that subexpression as a pattern, until all arguments have been covered.\nIf grind! is used, then only minimal indexable subexpressions are considered.\n\nBackward ReasoningThe ← modifier instructs grind to select a multi-pattern from the conclusion of theorem.\nIn other words, grind will use the theorem for backwards reasoning.\nThis may fail if not all of the arguments to the theorem appear in the conclusion.\nEach time it encounters a subexpression which covers an argument which was not\npreviously covered, it adds that subexpression as a pattern, until all arguments have been covered.\nIf grind! is used, then only minimal indexable subexpressions are considered.\n\nIt is important to inspect the patterns generated by the @[grind] attribute to ensure that they match the correct parts of the lemma.\nIf the pattern is too strict, the lemma will not be applied in situations where it would be relevant, leading to less automation.\nIf it is too general, then performance will suffer as the lemma is tried in many situations where it is not helpful.\n\nThere are also three less commonly used modifiers for lemmas:\n\nLeft-to-Right TraversalThe ⇒ modifier instructs grind to select a multi-pattern by traversing all the hypotheses from\nleft to right, followed by the conclusion.\nEach time it encounters a subexpression which covers an argument which was not\npreviously covered, it adds that subexpression as a pattern, until all arguments have been covered.\nIf grind! is used, then only minimal indexable subexpressions are considered.\n\nRight-to-Left TraversalThe ⇐ modifier instructs grind to select a multi-pattern by traversing the conclusion, and then\nall the hypotheses from right to left.\nEach time it encounters a subexpression which covers an argument which was not\npreviously covered, it adds that subexpression as a pattern, until all arguments have been covered.\nIf grind! is used, then only minimal indexable subexpressions are considered.\n\nBackward Reasoning on EqualityThe ←= modifier is unlike the other grind modifiers, and it used specifically for\nbackwards reasoning on equality. When a theorem's conclusion is an equality proposition and it\nis annotated with @[grind ←=], grind will instantiate it whenever the corresponding disequality\nis assumed—this is a consequence of the fact that grind performs all proofs by contradiction.\nOrdinarily, the grind attribute does not consider the = symbol when generating patterns.\n\nThe @[grind ←=] AttributeWhen attempting to prove that a⁻¹ = b, grind uses inv_eq due to the @[grind ←=] annotation.@[grind ←=]\ntheorem inv_eq [One α] [Mul α] [Inv α] {a b : α}\n    (w : a * b = 1) : a⁻¹ = b :=\n  sorry\n\n\nSome additional modifiers can be used to add other kinds of lemmas to the index.\nThis includes extensionality theorems, injectivity theorems for functions, and a shortcut to add all constructors of an inductively defined predicate to the index.\n\nExtensionalityThe ext modifier marks extensionality theorems for use by grind.\nFor example, the standard library marks funext with this attribute.Whenever grind encounters a disequality a ≠ b, it attempts to apply any\navailable extensionality theorems whose matches the type of a and b.In addition, adding @[grind ext] to a structure registers a its extensionality theorem\n\nThe @[grind ext] Attributegrind does not automatically apply the η-equality rule for structures.\nPoint is a structure with two fields:structure Point where\n  x : Int\n  y : Int\nBy default, grind can't solve goals like this one:example (p : Point) : p = ⟨p.x, p.y⟩ := by grind\n`grind` failed\ncase grind\np : Point\nh : ¬p = { x := p.x, y := p.y }\n⊢ False\n[grind] Goal diagnostics\n  [facts] Asserted facts\n  [eqc] False propositions\nThis kind of goal may come up when proving theorems like the fact that swapping the fields of a point twice is the identity:def Point.swap (p : Point) : Point := ⟨p.y, p.x⟩\ntheorem swap_swap_eq_id : Point.swap ∘ Point.swap = id := by\n  unfold Point.swap\n  grind\n`grind` failed\ncase grind\nh : ¬((fun p => { x := p.y, y := p.x }) ∘ fun p => { x := p.y, y := p.x }) = id\nw : Point\nh_1 : ¬{ x := w.x, y := w.y } = id w\n⊢ False\n[grind] Goal diagnostics\n  [facts] Asserted facts\n  [eqc] True propositions\n  [eqc] False propositions\n  [eqc] Equivalence classes\n  [cases] Case analyses\n  [ematch] E-matching patterns\n\n[grind] Diagnostics\nAdding the @[grind ext] attribute to Point enables grind to solve both the original example and prove this theorem:attribute [grind ext] Point\n\nexample (p : Point) : p = ⟨p.x, p.y⟩ := by\n  grind\n\ntheorem swap_swap_eq_id' : Point.swap ∘ Point.swap = id := by\n  unfold Point.swap\n  grind\n\n\nInjectivityThe inj modifier marks injectivity theorems for use by grind.\nThe conclusion of the theorem must be of the form Function.Injective f\nwhere the term f contains at least one constant symbol.\n\nInjectivity PatternsThis function double doubles its argument:def double (x : Nat) : Nat := x + x\nBy default, grind cannot prove the following theorem:theorem A {n k : Nat} :\n    double (n + 5) = double (k - 3) →\n    n + 8 = k := by\n  grind\nHowever, double is injective, and this fact can be registered for grind using the grind inj attribute:@[grind inj]\ntheorem double_inj : Function.Injective double := by\n  simp only [double, Function.Injective]\n  grind\nThis injectivity lemma suffices to prove the theorem:theorem B {n k : Nat} :\n    double (n + 5) = double (k - 3) →\n    n + 8 = k := by\n  grind\n\n\nConstructor PatternsThe intro modifier instructs grind to use the constructors (introduction rules)\nof an inductive predicate as E-matching theorems.Example:inductive Even : Nat → Prop where\n| zero : Even 0\n| add2 : Even x → Even (x + 2)\n\nattribute [grind intro] Even\nexample (h : Even x) : Even (x + 6) := by grind\nexample : Even 0 := by grind\nHere attribute [grind intro] Even acts like a macro that expands to\nattribute [grind] Even.zero and attribute [grind] Even.add2.\nThis is especially convenient for inductive predicates with many constructors.\n\nPatterns for ConstructorsThe predicate Decreasing states that each of the values in a list of integers is less than the one before, and the function decreasing checks this property, returning a Bool.inductive Decreasing : List Int → Prop\n  | nil : Decreasing []\n  | singleton : Decreasing [x]\n  | cons : Decreasing (x :: xs) → y > x → Decreasing (y :: x :: xs)\n\ndef decreasing : List Int → Bool\n  | [] | [_] => true\n  | y :: x :: xs => y > x && decreasing (x :: xs)\nThe function is correct if it returns true exactly when Decreasing holds for its argument.\nAttempting to prove this fact using a combination of fun_induction and grind fails immediately, with none of the three cases proven:def decreasingCorrect : decreasing xs = Decreasing xs := by\n  fun_induction decreasing <;> grind\n`grind` failed\ncase grind\nh : (true = true) = ¬Decreasing []\n⊢ False\n[grind] Goal diagnostics\n  [facts] Asserted facts\n  [eqc] True propositions\n  [eqc] False propositions\n`grind` failed\ncase grind\nhead : Int\nh : (true = true) = ¬Decreasing [head]\n⊢ False\n[grind] Goal diagnostics\n  [facts] Asserted facts\n  [eqc] True propositions\n  [eqc] False propositions\n`grind` failed\ncase grind.1\ny x : Int\nxs : List Int\nih1 : (decreasing (x :: xs) = true) = Decreasing (x :: xs)\nh : (-1 * y + x + 1 ≤ 0 ∧ decreasing (x :: xs) = true) = ¬Decreasing (y :: x :: xs)\nleft : -1 * y + x + 1 ≤ 0\nleft_1 : decreasing (x :: xs) = true\nright_1 : ¬Decreasing (y :: x :: xs)\n⊢ False\n[grind] Goal diagnostics\n  [facts] Asserted facts\n  [eqc] True propositions\n  [eqc] False propositions\n  [eqc] Equivalence classes\n  [cases] Case analyses\n  [cutsat] Assignment satisfying linear constraints\nAdding the grind intro attribute to Decreasing results in E-matching patterns being added for each of the three constructors, after which grind can prove the first two goals, and requires only a case analysis of a hypothesis to prove the final goal:attribute [grind intro] Decreasing\n\ndef decreasingCorrect' : decreasing xs = Decreasing xs := by\n  fun_induction decreasing <;> try grind\n  case case3 y x xs ih =>\n    apply propext\n    constructor\n    . grind\n    . intro\n      | .cons hDec hLt =>\n        grind\nAdding grind cases to Decreasing enables this case analysis automatically, resulting in a fully automatic proof:attribute [grind cases] Decreasing\n\ndef decreasingCorrect'' : decreasing xs = Decreasing xs := by\n  fun_induction decreasing <;> grind\n\n\n\n\n","context":"Lean Reference\u0009The  grind  tactic\u0009E‑matching","header":"17.6.1. Patterns","id":"/The--grind--tactic/E___matching/#e-matching-patterns"},"/The--grind--tactic/Linear-Integer-Arithmetic/#The-Lean-Language-Reference--The--grind--tactic--Linear-Integer-Arithmetic--Algebraic-Processing":{"contents":"The cutsat solver normalizes commutative (semi)ring expressions.\n\nCommutative (Semi)ring NormalizationCommutative ring normalization allows this goal to be solved:example (a b : Nat)\n    (h₁ : a + 1 ≠ a * b * a)\n    (h₂ : a * a * b ≤ a + 1) :\n    b * a ^ 2 < a + 1 := by\n  grind\n\n\n","context":"Lean Reference\u0009The  grind  tactic\u0009Linear Integer Arithmetic","header":"17.7.4. Algebraic Processing","id":"/The--grind--tactic/Linear-Integer-Arithmetic/#The-Lean-Language-Reference--The--grind--tactic--Linear-Integer-Arithmetic--Algebraic-Processing"},"/releases/v4.19.0/#The-Lean-Language-Reference--Release-Notes--Lean-4___19___0-_LPAR_2025-05-01_RPAR_--Library--Tree-Map":{"contents":"* #7270 provides lemmas about the tree map functions foldlM, foldl,\nfoldrM and foldr and their interactions with other functions for\nwhich lemmas already exist. Additionally, it generalizes the\nfold*/keys lemmas to arbitrary tree maps, which were previously\nstated only for the DTreeMap α Unit case.* #7331 provides lemmas about the tree map function insertMany and its\ninteraction with other functions for which lemmas already exist. Most\nlemmas about ofList, which is related to insertMany, are not\nincluded.* #7360 provides lemmas about the tree map function ofList and\ninteractions with other functions for which lemmas already exist.* #7367 provides lemmas for the tree map functions alter and modify\nand their interactions with other functions for which lemmas already\nexist.BREAKING CHANGE: The signature of size_alter was corrected for all four hash map types. Instead of relying on the boolean operations contains and && in the if statements, we now use the Prop-based operations Membership and And.* #7412 provides lemmas about the tree map that have been introduced to\nthe hash map in #7289.* #7419 provides lemmas about the tree map function modify and its\ninteractions with other functions for which lemmas already exist.* #7437 provides (some but not all) lemmas about the tree map function\nminKey?.* #7556 provides lemmas about the tree map function minKey? and its\ninteraction with other functions for which lemmas already exist.* #7600 provides lemmas about the tree map function minKey! and its\ninteractions with other functions for which lemmas already exist.* #7626 provides lemmas for the tree map function minKeyD and its\ninterations with other functions for which lemmas already exist.* #7657 provides lemmas for the tree map function maxKey? and its\ninterations with other functions for which lemmas already exist.* #7660 provides lemmas for the tree map function minKey and its\ninterations with other functions for which lemmas already exist.* #7664 fixes a bug in the definition of the tree map functions maxKey\nand maxEntry. Moreover, it provides lemmas for this function and its\ninteractions with other function for which lemmas already exist.* #7674 add missing lemmas about the tree map: minKey* variants return\nthe head of keys, keys and toList are ordered and getKey* t.minKey? equals the minimum.* #7675 provides lemmas about the tree map function maxKeyD and its\ninteractions with other functions for which lemmas already exist.* #7686 provides lemmas for the tree map function maxKey! and its\ninteractions with other functions for which lemmas already exist.* #7695 removes simp lemmas about the tree map with a metavariable in\nthe head of the discrimination pattern.* #7697 is a follow-up to #7695, which removed simp attributes from\ntree map lemmas with bad discrimination patterns. In this PR, we\nintroduce some Ord-based lemmas that are more simp-friendly.\n\n","context":"Lean Reference\u0009Release Notes\u0009Lean 4.19.0 (2025-05-01)\u0009Library","header":"Tree Map","id":"/releases/v4.19.0/#The-Lean-Language-Reference--Release-Notes--Lean-4___19___0-_LPAR_2025-05-01_RPAR_--Library--Tree-Map"},"/releases/v4.23.0/#The-Lean-Language-Reference--Release-Notes--Lean-4___23___0-_LPAR_2025-09-15_RPAR_--Highlights":{"contents":"Lean v4.23.0 release brings significant performance improvements, better error messages,\nand a plethora of bug fixes, refinements, and consolidations in grind, the compiler, and other components of Lean.\n\nIn terms of user experience, noteworthy new features are:\n\n* Improved 'Go to Definition' navigation (#9040)* Using 'Go to Definition' on a type class projection now extracts\nthe specific instances that were involved and provides them as locations\nto jump to. For example, using 'Go to Definition' on the toString of\ntoString 0 yields results for ToString.toString and ToString Nat.* Using 'Go to Definition' on a macro that produces syntax with type\nclass projections now also extracts the specific instances that were\ninvolved and provides them as locations to jump to. For example, using\n'Go to Definition' on the + of 1 + 1 yields results for\nHAdd.hAdd, HAdd α α α and Add Nat.* Using 'Go to Declaration' now provides all the results of 'Go to\nDefinition' in addition to the elaborator and the parser that were\ninvolved. For example, using 'Go to Declaration' on the + of 1 + 1\nyields results for HAdd.hAdd, HAdd α α α, Add Nat,\nmacro_rules | `($x + $y) => ... and infixl:65 \" + \" => HAdd.hAdd.* Using 'Go to Type Definition' on a value with a type that contains\nmultiple constants now provides 'Go to Definition' results for each\nconstant. For example, using 'Go to Type Definition' on x for x : Array Nat\nyields results for Array and Nat.* Interactive code-action hints for errors:* for \"invalid named argument\" error, suggest valid argument names (#9315)* for \"invalid case name\" error, suggest valid case names (#9316)* for \"fields missing\" error in structure instances, suggest to insert all the missing fields (#9317)You can try all of these in the Lean playground.\n\n\n\n","context":"Lean Reference\u0009Release Notes\u0009Lean 4.23.0 (2025-09-15)","header":"Highlights","id":"/releases/v4.23.0/#The-Lean-Language-Reference--Release-Notes--Lean-4___23___0-_LPAR_2025-09-15_RPAR_--Highlights"}});